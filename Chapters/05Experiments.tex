\documentclass[../Main.tex]{subfiles}
\begin{document}
\subsection{Model comparison}
We begin experiments by comparing style transfer features and quality of
original and pruned network. We compare models on detailed styles with complex 
structure (Figure \ref{fig:mosaic1}) and minimalistic, low-entropy styles 
(Figure \ref{fig:mosaic2}). 
        \begin{figure}[h!]
        \centering
            \includegraphics[scale=0.2]{mosaic1.png}
            \caption{Comparison of original and pruned network on complex styles. 
            For every pair of rows original model's results are placed in the
            top row and pruned model's results in bottom row.
            }
            \label{fig:mosaic1}
        \end{figure}
As can be seen, pruned model performs on par with the original on complex styles.
The only difference we notice for the first style is slightly lighter hue of sofa
in the dog photo. Otherwise it's hard to find any differences in this column.
In the second column the red jackets in kayak image stand out. This certainly is a drawback,
considering image was supposed to imitate a sketch. Area under a bridge in last 
image is also less detailed and darker. In general small model, as expected,
seems more likely to discard small details - compare surface of water in kayak image
or elephant's back in first image. For styles with one dominating color, smaller
model produces more uniform distribution of colors. It's visible in virtually
all images in the last column; in this case images seem to be "watered down".
Both of these traits are not necessarily drawbacks, for example the blue area
under the bridge in last image of last column looks less coherent than darker version
produced by small model. Discarding details might even be desirable for many styles.

        \begin{figure}[h!]
        \centering
            \includegraphics[scale=0.2]{mosaic2.png}
            \caption{Comparison of original and pruned network on simple styles. 
            For every pair of rows original model's results are placed in the
            top row and pruned model's results in bottom row.
            }
            \label{fig:mosaic2}
        \end{figure} 
 
We now compare networks on very simple styles. In degenerate case of entirely black
style image they perform almost the same. Original model preserves a bit more 
structure, though it might not be visible. It makes sense results are "empty"
since black color is represented by $0$s and the first convolution layer amounts
just to it's bias. Pruned network performs visibly worse in the second case.
Very low contrast of images makes it hard to recognize the original image, while
the original model has no problem approximately reconstructing content. 
The third column is very informative - even though it consists mostly of 2 colors
(unlike a sketch image where various tones of grey are prevalent) both networks
reconstruct content very well. This tells us structure and presence of some kind
of edges in style image is very important for reconstruction - images in the last
column are more blurry, even though color information is richer.
Fourth column shows smaller model  doesn't work very well when structure of style 
image is very sparse. While original model acts as (not very good) edge detector,
pruned network produces seemingly overexposed images.
Overall pruned model performs visibly worse on styles with very simple structure or no
structure at all. 
\subsubsection{Speedup}
In this section we measure speedup provided by pruning and TensorRT. 
We compare speed gains across different devices by taking measurements on three different
NVIDIA GeForce GPUs of varying computational power. In all tests we use RGB images
of resolution 1024x576. 
\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
                  &  940M & 960 & 1080 Ti \\
\hline
Original model    &  0.75 & 5   & 23 \\
\hline
Pruned model      &  4.2  & 22  & 79 \\
\hline
Pruned + TensorRT &  7.1  & 26  & 70 \\
\hline
\end{tabular}
\end{center}
\label{table:speedup}
\end{table}
As we see speedup gained by pruning the original model is tremendous on
every architecture, though it decreases with computational power.
For pruned models using TensorRT this trend is even more emphasized.
While the mobile 940M GPU speeds up by 75\%, middle-range 960 only gains
couple FPS, which at that point don't really make any differnece. The most
interesting case is that of high-end GTX 1080 Ti, which slows down
when TensorRT is used. This might be exactly the result of it's computational power.
Pruned model is no longer bottlenecked by factors, the two weaker GPUs
introduced and as a consequence it can be ran very efficiently.
Since there is nothing to improve, tensor reshapes requiered by TensorRT
might slow down inference.

\subsection{Style scaling}
We can control how much the result video or photo will be styled. For this purpose we added factor $\alpha$ -  which is style weight set in mobile application. Using method of style scaling based on interpolation between parameters of AdaIN given as input to decoder, what can be present as following equation: $$T(c,s,a) = g((1-\alpha)f(c)+\alpha AdaIN(f(c),f(s)))$$ When $\alpha= 0$  then the network will reconstruct the content style, $\alpha = 1$ is maximum level of styled, which was default style weight in previous experiments. 

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.75]{Images/style1.png}
    \caption{Comparison between different values of style weight}
    \label{fig:style1}
\end{figure}

\subsection{Color preservation}
\subsection{Video stability}
One of goals for high-quality style transfer is stability. Let $F_1$ and $F_2$ be 
two video frames relatively close in time and $T$ a style transfer algorithm transforming
a single frame. We can define stability of $T$ by mean difference between
$| F_1 - F_2 |$ and $|T(F_1)-T(F_2)|$. The smaller that difference, the more stable $T$ is.
This definition says that if a certain region doesn't change between $F_1$ and $F_2$, then
it also shouldn't change between $T(F_1)$ and $T(F_2)$, that is the transformed video
shouldn't flicker too much. Definition also implies changes in orgiginal video
should be reflected in the stylized video, but it's an obvoiys requirement and it's
the former property that we usually call stability.
Stability can be expilicitly guaranteed by use of special loss functions
and proper training procedure \cite{stability1, stability2}. The model we base our work
on however learns stability by design \cite{Li2018}, so there is no need for
any auxillary techniques. We examine to what extent network pruning damages this 
property. We conduct the experiment using a single style image and 3 videos with motionless
camera. Results are presented in Figure \ref{fig:heatmap}.

\begin{figure}[h!]
\centering
    \includegraphics[scale=0.2]{heatmap.jpg}
    \caption{Comparison of stability between original (middle column) and pruned
    (right column) model. Left column shows original video frames. For each of
    $3\times3$ grids first two rows show two frames close two each other
    ($1$ to $5$ frames of difference, depending on video's framerate) and the third
    row is heatmap of difference between them. The image on the right is style used
    throughout experiment; below it heatmap scale.
    }
    \label{fig:heatmap}
\end{figure} 

For each of $3\times3$ grids, the third row shows heatmaps of differences between 
two images above it. A hypothetic perfectly stable style transform algorithm would
produce the same heatmaps as the ones in left column. While both models perform
resaonbly well for second and third video, they completely fail in the first case.
The reason for such behaviour is progressive change of brightness of the sky
which can't be seen in shown frames. Sky's brightness changes significantlly 
as a result of some vehicle driving in front of camera. In other words
stylizing poorly recorded video results in instability and noise. Comparing
original and pruned model, we also see the latter is less stable for this video,
though stripes produced by original model may seem even worse than overall
noise outputed by pruned model. In the second case both networks perform equally well
and their heatmaps are only slightly more noisy than the original heatmap.
For the third video pruned model is only a bit more unstable than original model.
Overall pruning seems to reduce stablility only by a small margin provided original network
preserves stability. In situation where original network fails to do so, pruned model
magnifies errors though.


\subsection{Style and content encoding} \label{two_encoders}
As mentioned in section \ref{network} the encoder had to be copied due to pruning
constraints. Since two resulting encoders were pruned and fine-tuned separately 
their sets of weights are different. In this section we shortly examine effects
of using wrong combinations of encoder in pruned model. Results for four possible combinations
of encoders are presented if Figure \ref{fig:mosaic_asym}.

        \begin{figure}[h!]
        \centering
            \includegraphics[scale=0.2]{mosaic_asym.png}
            \caption{Comparison of various configurations of network with respect
            to encoder used for encoding style/content image. From top to bottom:
            $S_E(S_I)$ and $C_E(C_I)$, $S_E(S_I)$ and $S_E(C_I)$,
            $C_E(S_I)$ and $S_E(C_I)$, $C_E(S_I)$ and $C_E(C_I)$, where
            $S_E$ and $S_C$ are style and content encoders, $S_I$ and $C_I$ are 
            style and content images.
            }
            \label{fig:mosaic_asym}
        \end{figure}
        
The first row presents default variant with proper configuration. 
We see that while style encoder $S_E$ is able to encode content features to some
degree (2nd row), content encoder $C_E$ fails completely at style extraction
(3rd and 4th row). This is expected, since $C_E$ can usually safely
discard most of color information in content image. On the other hand, $S_E$
should pay attention to both color and shapes. Consequently images in second row
are of a lot worse quality than first row, but content and style are still readily
recognizable. Comparing 3rd and 4th row, we see content 
is reconstructed better in the latter one. Again, this should be the case,
since content is encoded with $S_E$ in 3rd row and with $C_E$ in 4th row.
Since $C_E$ produces more accurate shape encoding than $S_E$, images in 4th row are
sharper and less blurry.





\biblio % Needed for referencing to working when compiling individual subfiles - Do not remove
\end{document}
