\documentclass[../Main.tex]{subfiles}
\begin{document}

\subsection{What is style transfer}

\subsection{Machine Learning} 
% or DEEP LEARNING
%\bartek[inline]{write intros based on artciles listed below}
    \subsubsection{What is Machine Learning?}
    see chapter 1.1 in \\
    \url{https://www.bc.edu/content/dam/bc1/schools/mcas/cs/pdf/honors-thesis/Aleshin-Guendel_Serge_Thesis.pdf} \\
    see chapter 2.1.1 in \\
    \url{https://www.ke.tu-darmstadt.de/lehre/arbeiten/master/2016/Muenker_Christoph.pdf} \\
    check \\
    \url{https://www.ics.ei.tum.de/fileadmin/w00bcw/www/ics/pics/research/data_sets/MasterarbeitICS-FINALVERSION-Niklas.pdf}\\


\subsection{Neural Networks}
    \subsubsection{Overview}
    \subsubsection{Layers}
        \textbf{Fully Connected Layers}
        \textbf{Convolutional Layers}
    \subsubsection{Loss} %for style tranfer
    \subsubsection{Activation}
        \textbf{ReLU}
    \subsubsection{Backpropagation}

\subsection{Neural network compression}
    Latest neural networks usually have between 2 million and 50 million parameters.
    Older architectures are even heavier - full VGG16 model has as much as
    138 million parameters. In consequence models 
    are often too big for deployment on memory bounded mobile and embedded devices.
    Number of parameters strongly correlates with inference time, which in turn
    prevents real-time inference not only on embedded and mobile
    devices but even on middle-class GPUs. Overcoming these limitations 
    is very active area of research.\\
    \textbf{Specialized architectures} such as MobileNet [\cite{mobilenetv1},
    \cite{mobilenetv2}, \cite{mobilenetv3}] and ShuffleNet [\cite{shufflenetv1},
    \cite{shufflenetv2}], replace full convolutions with bottlenecks of lighweight
    pointwise, depthwise and group convolutions in order to reduce number of 
    arithmetic operations and parameters. MobileNetV3 \cite{mobilenetv3} uses
    Neural Architecture Search to optimize the network for mobile phone CPU
    inference, while discussion in \cite{shufflenetv2} shows what aspects should
    be considered, when designing mobile architecture manually. 
    \textbf{Network pruning} aims to reduce already trained model's size by pruning away
    weights with the least impact on network's quality. [\cite{zhu2017prune}] show
    that for some models even up to $87.5$ weights can be removed with only 
    marginally reduced performance. 
    Because networks are initialized randomly, the least
    important parameters are usually spread across whole network. Naive pruning 
    then results in sparse networks. Their storage is significantly reduced, however
    because available linear algebra libraries are optimized for dense structures,
    their inference time doesn't scale as well. To overcome this, more structured
    methods of pruning were developed, among them filter pruning [\cite{li2016pruning},
    \cite{molchanov2016pruning}]
    and channel pruning [\cite{he2017channel}]. 
    Weight's or weights set's importance can be measured by various heuristics. 
    [\cite{li2016pruning}] prune away the filters with the smallest $\ell_1$ norm.
    This approach easily generalizes to $\ell_p$ norm.
    [\cite{polyak2015}] choose to prune away the filters with
    the smallest activation statistics. More direct methods formulate pruning 
    try to preserve performance metrics (e.g. low loss, high accuracy) on training set. 
    [\cite{molchanov2016pruning}] formulate pruning as optimization problem where
    the goal is preserving original training set loss value, that is minimizing
    $|L_{new} - L_{orig}|$. Curiously changing the goal to $L_{new} - L_{orig}$,
    that is letting the loss drop, degrades the resulting network's performance.\\
    Smaller networks can also be trained with aid of larger ones through process called
    \textbf{knowledge distillation} [\cite{hinton2015distilling}]. 
    The key idea is that activations of certain layers
    contain knowledge which is not present in dataset's labels. For example in network 
    trained for classification, the final softmax layer's outputs are usually 
    interpreted as probabilities of respective classes. These probabilities might indicate,
    that for given input x, $C_1$ is the proper class and class $C_2$ is twice as probable 
    as class $C_3$. If the network is well trained, this means x is more similar to
    objects of class $C_2$, rather than $C_3$. By training new small network to match both
    it's softmax outputs with larger trained network's softmax outputs and proper labels,
    we effectively gain additional training data and ease the training.
    Almost every network's inference time can also be increased by \textbf{quantization}.
    In many deep learning frameworks, computation is by default performed on
    32-bit floating point numbers. Such precision is often required during training to 
    ensure gradient's numerical stability. However given that network's weights have
    relatively small magnitude, in many applications it's unnecessary during inference.
    Modern GPUs enable inference in FP16 and INT8 modes. There has also been work 
    on even lower precision inference, performed on CPUs. 
    %TODO: opisać z tej strony: https://software.intel.com/en-us/articles/lower-numerical-precision-deep-learning-inference-and-training; napisać o obecnym stanie bibliotek co
    % do wsparcia dla kwantyzacji
    
    

\subsection{Image and Video Processing}
    \subsubsection{Basics}
%    \TODO {add subsections of Image and Video Processing}
    \subsubsection{Examples}


\biblio % Needed for referencing to working when compiling individual subfiles - Do not remove
\end{document}
